CS 111
Lab 4
Jason Liu & Byron Chien

QUESTIONS 1.1
1. Why does it take this many threads or iterations?
When there is more than 1 thread, race conditions become a problem.
Particularly, two threads can read and write the counter variable at the same time.
As the number of operations increases, the chance for this to happen increases.

2. Why does a significantly smaller number of iterations so seldom fail?
If the number of iterations is low, threads are likely to finish quickly.
This reduces the possibility of threads reading/writing to counter at the same time.


QUESTIONS 1.2
1. Why does the average cost per operation drop with increasing iterations?
The cost of creating a thread and context swtiching into a thread is high.
If there are not many iterations, that cost is high compared to the cost of the operations.
You only need to create a thread once before it can just do its job.
Therefore, increasing iterations drops the average cost per operation.

2. How do we know what the “correct” cost is?
Once we have a proper number of iterations, the cost of thread creation becomes negligible.
We can take this as the "correct" cost.

3. Why are the --yield runs so much slower? Where is the extra time going?
Yield forces a context swtich every time the add function is called.
The extra time comes from having to switch between running each thread/

4. Can we get valid timings if we are using --yield? How, or why not?
We can get valid timings by measuring the time it takes to context switch.
If we add up all those values, we can subtract it from the final timing.


QUESTIONS 1.3
1. Why do all of the options perform similarly for low numbers of threads?
Low numbers of threads means that there are less race conditions.
Therefore, mutexes and locking mechanisms don't block threads for nearly as long.

2. Why do the three protected operations slow down as the number of threads rises?
Similarly, as the number of threads rises, there are many races.
Therefore, each protected operation must block threads from running until others finish.
This casuses a lot of threads to be waiting for their turn, increasing elapsed time.

3. Why are spin-locks so expensive for large numbers of threads?
For each thread, they must get the lock to continue execution of a critical section.
There is only 1 lock, but a large number of threads.
The way spin-locks are implemented, each thread must continously try to get the lock.
This means while a lot of threads are stuck waking up and yielding,
only one thread is actually getting work done.


QUESTIONS 2.1
Explain the variation in time per operation vs the number of iterations? How would you
propose to correct for this effect?
This variation is the same as the variation in time per operation in exercise 1. If there
are few iterations, the cost of creating the threads drives the average cost of operations
up. By increasing the number of operations, the true average cost per operation can be
determined by diluting the effect of creating threads.

QUESTIONS 2.2
Compare the variation in time per protected operation vs the number of threads in Part 2
and in Part 1. Explain the difference.
The variation in time per protected operation in Part 2 and Part 1 for the same number of
iterations and threads suggests better performance in Part 2. However, this variation is a
result of the cost of creating threads such as in questions 2.1 and 1.2.1. Since there are
more operations per iteration in part 2, the effect of creating threads is diluted fast
resulting in what appears to be better performance.


QUESTIONS 2.3
1. Explain the the change in performance of the synchronized methods as a function of the
number of threads per list.
The performance of synchronized methods is more influenced by the number of lists used
than the ratio of threads to list since the cost per operation is higher with more lists
for the same ratio of threads to lists.

2. Explain why threads per list is a more interesting number than threads (for this
particular measurement).
Threads per list is a more interesting way to measure performance since it shows how the
performance is affected both the number of lists and the number of threads. In this case,
both increase the cost per operation, but the number of threads affects performance more.

QUESTIONS 3-1
1. Why must the mutex be held when pthread_cond_wait is called?
The function needs to be sure the condition is evaluating can't be changed during evaluation.
This can prevent race conditions.

2. Why must the mutex be released when the waiting thread is blocked?
While blocked, the waiting thread is waiting for the condition to become true.
If it doesn't release the mutex, it will begin waiting, but the conditional variables will still be locked.

3. Why must the mutex be reacquired when the calling thread resumes?
For the same reason as the first question.
The thread must be able to safely evaluate the condition.

4. Why must this be done inside of pthread_cond_wait? Why can't the caller simply release
the mutex before calling pthread_cond_wait?
Let's look at this code:
while(!condition) {
   pthread_cond_wait(&condvar, &mut);
}
Say pthread_cond_wait doesn't automatically release/reacquire the mutex; then we would need:
while(!condition) {
   pthread_mutex_unlock(&mut);
   pthread_cond_wait(&condvar); // doesn't release/reacquire mutex
   pthread_mutex_lock(&mut);
}
Between manually unlocking and locking the mutex, the conditional variables can be modified.
This can cause unnessecary waiting.

5. Can this be done in a user-mode implementation of pthread_cond_wait? If so, how? If it can
only be implemented by a system call, explain why?
No. It can only be implemented by a system call since when acquiring/releasing a mutex, it must be done atomically.


PART 4
During the lab, we saw that doing a few number of iterations results in a fairly ridiculous operation time average.
We suggested that much of this is due to overhead, such as thread creation/deletion, context switching, etc.
There are a lot of things that need to be calulated only once regardless of the number of operations.
Because of where we were asked to collect timestamps, much of this overhead was including in the final time.

By increasing the number of operations to a large number, the effects of this overhead is made negligible.

We attempted to correct this and make a low number of iterations still get a fairly accurate operation time.
To do this, we only collected timestamps while doing the actual operations (add, insert, delete functions, etc).
We included an option --correct to enable this behavior, else the program will behave as before.
We also wanted --correct to be fairly accurate for a large number of iterations.

More detailed implementation:
We have a global variable time_design that is initialized to 0.
Every time we call a function that qualifies as an operation, we start the time.
We then immediately collect the end time.
The time difference is then added to time_design via an atomic operation.

Here's some data that shows the improvement:

PART 1
iter=1
	Original:  74362 ns
	Corrected: 283 ns
iter=100
	Original:  709 ns
	Corrected: 22 ns
iter=100000
	Original:  49 ns
	Corrected: 21 ns


PART 2
iter=1
	Original:  163121 ns
	Corrected: 14720 ns
iter=100
	Original:  35 ns
	Corrected: 16 ns
iter=10000
	Original:  7 ns
	Corrected: 7 ns


We can see that our solution improved the operation timing by several orders of magnitude for low iterations,
while keeping the timing for large number of iterations true.

Note that these readings become even better when using multiple threads.
If a processer is running the program multithreaded, the timing can be too short since parallelism.
Our solution fixes this by measuring every operation in each thread before taking the average.
